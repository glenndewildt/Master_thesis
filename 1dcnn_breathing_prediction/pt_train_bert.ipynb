{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n",
      "(864, 480000)\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/gdwildt.9115475/ipykernel_149991/51500724.py:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.float16):\n",
      "/home/gdwildt/.conda/envs/thesis/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/scratch-local/gdwildt.9115475/ipykernel_149991/51500724.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/100:   0%|          | 0/124 [00:00<?, ?it/s]/scratch-local/gdwildt.9115475/ipykernel_149991/51500724.py:177: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.float16):\n",
      "/home/gdwildt/.conda/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/100: 100%|██████████| 124/124 [02:00<00:00,  1.03it/s, train_loss=0.9970]\n",
      "/scratch-local/gdwildt.9115475/ipykernel_149991/51500724.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.float16):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torchaudio\n",
    "from typing import List, Tuple\n",
    "from pt_utils import *\n",
    "from pt_dataset import *\n",
    "from pt_models import *\n",
    "from pt_utils import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "def create_run_directory():\n",
    "    base_dir = \"pt_runs\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(base_dir, timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def _calculate_flattened_accuracy(average, ground_truth_labels):\n",
    "    s_acc = 0\n",
    "    for b in range(len(ground_truth_labels)):\n",
    "        s, _ = scipy.stats.pearsonr(average[b], ground_truth_labels[b])\n",
    "        s_acc += s\n",
    "    return s_acc / len(ground_truth_labels)\n",
    "\n",
    "def _choose_real_labs_only_with_filenames(labels, filenames):\n",
    "    return labels[labels['filename'].isin(filenames)]\n",
    "\n",
    "def _get_ground_truth_labels(ground_truth_names, labels):\n",
    "    ground_truth_labels = []\n",
    "    for batch_name in ground_truth_names:\n",
    "        ground_truth_label = _choose_real_labs_only_with_filenames(labels, [batch_name])\n",
    "        ground_truth_labels.append(ground_truth_label)\n",
    "    return np.array(ground_truth_labels)[:, :, -1].astype(np.float32)\n",
    "# Function to gradually unfreeze layers\n",
    "\n",
    "def gradually_unfreeze(model, epoch, total_epochs, initial_unfreeze=0):\n",
    "    total_layers = len(model.wav_model.encoder.layers)\n",
    "    layers_to_unfreeze = int(initial_unfreeze + (epoch * (total_layers - initial_unfreeze) // total_epochs))\n",
    "    print(f\"flayers_to_unfreeze : {layers_to_unfreeze}\")\n",
    "    model.unfreeze_last_n_blocks(layers_to_unfreeze)\n",
    "\n",
    "def train(path_to_data, path_to_labels, window_size=16, step_size=6, data_parts=4, epochs=100, batch_size=10, early_stopping_patience=20, config = None, model =None, processor = None):\n",
    "    run_dir = create_run_directory()\n",
    "    log_dir = os.path.join(run_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Parameters\n",
    "    length_sequence = window_size \n",
    "    step_sequence = step_size\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_data, train_labels, train_dict, frame_rate = load_data(path_to_data, path_to_labels, 'train')\n",
    "    devel_data, devel_labels, devel_dict, frame_rate = load_data(path_to_data, path_to_labels, 'devel')\n",
    "    test_data, test_labels, test_dict, frame_rate = load_data(path_to_data, path_to_labels, 'test')\n",
    "\n",
    "    # Combine train and devel data\n",
    "    all_data = np.concatenate((train_data, devel_data), axis=0)\n",
    "    all_labels = pd.concat([train_labels, devel_labels])\n",
    "    all_dict = np.concatenate((list(train_dict.values()), list(devel_dict.values())), axis=0)\n",
    "\n",
    "    # Prepare data\n",
    "    prepared_data, prepared_labels, prepared_labels_timesteps = prepare_data(all_data, all_labels, all_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "    prepared_test_data, prepared_test_labels, prepared_test_labels_timesteps = prepare_data(test_data, test_labels, test_dict, frame_rate, length_sequence * 16000, 6 * 16000)\n",
    "\n",
    "    # Create CSV file for storing fold indices\n",
    "    fold_indices_df = pd.DataFrame(columns=['Fold', 'Train_Indices', 'Val_Indices'])\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = KFold(n_splits=data_parts)\n",
    "    fold_metrics = []\n",
    "    # To accumulate metrics across folds for each epoch\n",
    "    train_acc_epoch = []\n",
    "    val_acc_epoch = []\n",
    "    test_acc_epoch = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    config[\"output_size\"] = prepared_labels.shape[-1]\n",
    "    writer = SummaryWriter(log_dir=os.path.join(log_dir,config[\"model_name\"]))\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(prepared_data)):\n",
    "        # if fold < 3:\n",
    "        #     continue\n",
    "        print(f\"Fold {fold + 1}/{data_parts}\")\n",
    "        best_model_path = f\"{run_dir}/best_model_fold{fold+1}\"\n",
    "        # Save fold indices\n",
    "        fold_indices_df = fold_indices_df._append({\n",
    "            'Fold': fold + 1,\n",
    "            'Train_Indices': train_index.tolist(),\n",
    "            'Val_Indices': val_index.tolist()\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Split data\n",
    "        train_d, val_d = prepared_data[train_index], prepared_data[val_index]\n",
    "        train_lbs, val_lbs = prepared_labels[train_index], prepared_labels[val_index]\n",
    "        train_timesteps, val_timesteps = prepared_labels_timesteps[train_index], prepared_labels_timesteps[val_index]\n",
    "        \n",
    "        # Reshape data\n",
    "        train_d, train_lbs = reshaping_data_for_model(train_d, train_lbs)\n",
    "        val_d, val_lbs = reshaping_data_for_model(val_d, val_lbs)\n",
    "        test_d, test_lbs = reshaping_data_for_model(prepared_test_data, prepared_test_labels)\n",
    "        \n",
    "        print(train_d.shape)\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = GPUBreathingDataset(train_d, train_lbs, processor, augment=True)\n",
    "        #train_dataset = BreathingDataset(train_d, train_lbs, processor,window_size, step_sequence)\n",
    "        val_dataset = BreathingDataset(val_d, val_lbs, processor, window_size, step_sequence)\n",
    "        test_dataset = BreathingDataset(test_d, test_lbs, processor, window_size, step_sequence)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collate_fn)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)\n",
    "        print(config[\"output_size\"])\n",
    "        # Create and initialize model\n",
    "        with autocast(dtype=torch.float16):\n",
    "            model = config[\"model\"](config).to(device)\n",
    "        ## uses scadular and optimiser from the parems\n",
    "        # Training optimizer parameters\n",
    "\n",
    "        # Optimizer: AdamW\n",
    "\n",
    "        # Define total steps and warmup steps\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "        # Scheduler: CosineAnnealingWarmRestarts\n",
    "        # scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "        #                                             num_warmup_steps=warmup_steps, \n",
    "        #                                             num_training_steps=total_steps)\n",
    "        patience = 15\n",
    "        learning_rate = 2e-5\n",
    "        weight_decay = 0.001\n",
    "\n",
    "        # Optimizer and scheduler parameters\n",
    "        t0 = 10\n",
    "        t_mult = 1\n",
    "        min_lr = 5e-6\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=t0, T_mult=t_mult, eta_min=min_lr)\n",
    "        scaler = GradScaler()        \n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        # To accumulate metrics across folds for each epoch\n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        test_acc = []\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            for input_values, batch_lbs in progress_bar:\n",
    "                input_values = {k: v.to(device).float() for k, v in input_values.items()}\n",
    "                batch_lbs = batch_lbs.to(device).float()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Autocast to train with a bigger batch size\n",
    "                with autocast(dtype=torch.float16):\n",
    "                    # Move data to device and convert to float16\n",
    "\n",
    "                    outputs = model(input_values)\n",
    "                    loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "                \n",
    "                # Scale loss and compute gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Unscale gradients and take optimizer step\n",
    "                scaler.unscale_(optimizer)\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                progress_bar.set_postfix({'train_loss': f'{train_loss/(progress_bar.n+1):.4f}'})\n",
    "        \n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            # Combined validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_pred = []\n",
    "            with torch.no_grad():\n",
    "                for batch_d, batch_lbs in val_loader:\n",
    "                    with autocast(dtype=torch.float16):\n",
    "        \n",
    "                        input_values = batch_d.to(device)\n",
    "                        batch_lbs = batch_lbs.to(device)\n",
    "                        \n",
    "                        outputs = model(input_values)\n",
    "                        loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "                    val_loss += loss.item()\n",
    "                    val_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            # Calculate validation metrics\n",
    "            val_pred = np.array(val_pred).reshape(val_timesteps.shape)\n",
    "            val_ground_truth = _get_ground_truth_labels([all_dict[i] for i in val_index], all_labels)\n",
    "            val_pred_flat = unsplit_data_ogsize(val_pred, window_size, step_sequence, 25, val_ground_truth.shape[-1])\n",
    "            val_prc_coef = _calculate_flattened_accuracy(val_pred_flat, val_ground_truth)\n",
    "            val_loss_flat = 1- val_prc_coef\n",
    "            \n",
    "            # Accumulate metrics for this fold and epoch\n",
    "            train_acc.append(1- train_loss)\n",
    "            val_acc.append(1- val_loss)\n",
    "\n",
    "            # Log metrics\n",
    "            writer.add_scalar(f\"Loss/train_fold_{fold + 1}\", train_loss, epoch)\n",
    "            writer.add_scalar(f\"Loss/val_fold_{fold + 1}\", val_loss, epoch)\n",
    "            writer.add_scalar(f\"Pearson/val_fold_{fold + 1}\", val_prc_coef, epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Pearson: {val_prc_coef:.4f}\")\n",
    "\n",
    "            # Check if validation loss improved\n",
    "            if val_loss_flat < best_val_loss:\n",
    "                print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving best model...\")\n",
    "                best_val_loss = val_loss_flat\n",
    "                early_stopping_counter = 0\n",
    "\n",
    "                # Save the best model\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                print(f\"Validation loss did not improve for {early_stopping_counter} epochs.\")\n",
    "                #model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "\n",
    "            # Early stopping\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}. Loading best model.\")\n",
    "                # Load the best model's weights\n",
    "                break\n",
    "\n",
    "            # Adjust the learning rate based on validation loss\n",
    "\n",
    "            #scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        test_pred = []\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_pred = []\n",
    "            for batch_d, batch_lbs in val_loader:\n",
    "                with autocast(dtype=torch.float16):\n",
    "    \n",
    "                    input_values = batch_d.to(device)\n",
    "                    batch_lbs = batch_lbs.to(device)\n",
    "                    \n",
    "                    outputs = model(input_values)\n",
    "                    loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "                val_loss += loss.item()\n",
    "                val_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "            \n",
    "            for batch_d, batch_lbs in test_loader:\n",
    "                with autocast(dtype=torch.float16):\n",
    "        \n",
    "                    input_values = batch_d.to(device)\n",
    "                    batch_lbs = batch_lbs.to(device)\n",
    "                    \n",
    "                    outputs = model(input_values)\n",
    "                    loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "                test_loss += loss.item()\n",
    "                test_pred.extend(outputs.cpu().numpy())\n",
    "                \n",
    "        val_loss /= len(val_loader)\n",
    "        # Calculate validation metrics\n",
    "        val_pred = np.array(val_pred).reshape(val_timesteps.shape)\n",
    "        val_ground_truth = _get_ground_truth_labels([all_dict[i] for i in val_index], all_labels)\n",
    "        val_pred_flat = unsplit_data_ogsize(val_pred, window_size, step_sequence, 25, val_ground_truth.shape[-1])\n",
    "        val_prc_coef = _calculate_flattened_accuracy(val_pred_flat, val_ground_truth)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_pred = np.array(test_pred).reshape(prepared_test_labels_timesteps.shape)\n",
    "        test_ground_truth = _get_ground_truth_labels(list(test_dict.values()), test_labels)\n",
    "        test_pred_flat = unsplit_data_ogsize(test_pred, window_size, 6, 25, test_ground_truth.shape[-1])\n",
    "        test_prc_coef = _calculate_flattened_accuracy(test_pred_flat, test_ground_truth)\n",
    "\n",
    "        print(f\"Fold {fold + 1}:\")\n",
    "        print(f\"  Validation Pearson Coefficient  acc: {1- val_loss}\")\n",
    "        print(f\"  Validation Pearson Coefficient flat acc: {val_prc_coef}\")\n",
    "        print(f\"  Test acc: {1- test_loss}\")\n",
    "        print(f\"  Test Pearson Coefficient acc(flattened): {test_prc_coef}\")\n",
    "\n",
    "        fold_metrics.append({\n",
    "            'Fold': fold + 1,\n",
    "            'val_prc_acc': 1- val_loss,\n",
    "            'val_prc_acc_flat': val_prc_coef,\n",
    "            'test_acc': 1- test_loss,\n",
    "            'test_prc_flat': test_prc_coef\n",
    "        })\n",
    "\n",
    "\n",
    "                # Log fold-specific metrics as tables\n",
    "        fold_table = f\"| Fold | Val Pearson Acc | Val Pearson Flat | Test Acc | Test Pearson Flat |\\n\" \\\n",
    "                     f\"|------|-----------------|------------------|----------|-------------------|\\n\" \\\n",
    "                     f\"| {fold + 1} | {1 - val_loss:.4f} | {val_prc_coef:.4f} | {1 - test_loss:.4f} | {test_prc_coef:.4f} |\\n\"\n",
    "        writer.add_text(f\"Fold_{fold + 1}_Metrics\", fold_table)\n",
    "        # Accumulate fold metrics across all folds\n",
    "        train_acc_epoch.append(train_acc)\n",
    "        val_acc_epoch.append(train_acc)\n",
    "\n",
    "\n",
    "    \n",
    "        # After all folds, compute and log the average metrics per epoch across all folds\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = np.mean([fold_losses[epoch] for fold_losses in train_acc_epoch if len(fold_losses) > epoch])\n",
    "        avg_val_loss = np.mean([fold_losses[epoch] for fold_losses in val_acc_epoch if len(fold_losses) > epoch])\n",
    "\n",
    "        # Log the averaged metrics for the epoch across all folds\n",
    "        writer.add_scalar(\"Average_acc/train\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"Average_acc/val\", avg_val_loss, epoch)\n",
    "            \n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics if key != 'Fold']) for key in fold_metrics[0].keys() if key != 'Fold'}\n",
    "        # Log the final average table\n",
    "    avg_table = \"| Fold | Val Pearson Acc | Val Pearson Flat | Test Acc | Test Pearson Flat |\\n\" \\\n",
    "                \"|------|-----------------|------------------|----------|-------------------|\\n\" \\\n",
    "                f\"| Average | {avg_metrics['val_prc_acc']:.4f} | {avg_metrics['val_prc_acc_flat']:.4f} | {avg_metrics['test_acc']:.4f} | {avg_metrics['test_prc_flat']:.4f} |\\n\"\n",
    "    writer.add_text(\"Average_Metrics\", avg_table)\n",
    "    # Add average metrics to results\n",
    "    avg_metrics['Fold'] = 'Average'\n",
    "    fold_metrics.append(avg_metrics)\n",
    "\n",
    "    # save averga date to CSV\n",
    "    results_df = pd.DataFrame(fold_metrics)\n",
    "    csv_path = os.path.join(run_dir, 'fold_results.csv')\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save fold indices CSV\n",
    "    fold_indices_df.to_csv(os.path.join(run_dir, 'fold_indices.csv'), index=False)\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(\"Average metrics across all folds:\")\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## Path to data\n",
    "    #path = \"/home/glenn/Downloads/\"\n",
    "    path = \"../../DATA/\"\n",
    "\n",
    "    path = \"../DATA/ComParE2020_Breathing/\"\n",
    "\n",
    "\n",
    "    # Model parameters\n",
    "    model_config = {\n",
    "        \"RespBertLSTM\": {\n",
    "            'model': RespBertLSTM,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "        \"RespBertAttention\": {\n",
    "            'model' : RespBertAttention,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 512,\n",
    "            \"n_attion\": 2,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "\n",
    "        \"RespBertCNN_12\": {\n",
    "            'model' : RespBertCNN,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"output_size\": None,\n",
    "            \"number_finetune\": 12 \n",
    "        },\n",
    "    \n",
    "        \"RespBertCNN_16\": {\n",
    "            'model' : RespBertCNN,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"output_size\": None ,\n",
    "            \"number_finetune\": 16 \n",
    "\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train and data parameters\n",
    "    epochs = 100\n",
    "    batch_size = 7\n",
    "    window_size = 30\n",
    "    step_size = 6\n",
    "    data_parts = 4 # aka folds\n",
    "    early_stopping_patience = 15\n",
    "    \n",
    "    config = model_config[\"RespBertLSTM\"]    \n",
    "    model = None\n",
    "    ## Wav2Vec2Feature extractor needs to be used for the WavLM models. The other models models can use the autoprocessor\n",
    "    #processor = AutoProcessor.from_pretrained(config[\"model_name\"])\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "\n",
    "    train(\n",
    "        path_to_data=path+\"/wav/\",\n",
    "        path_to_labels=path+\"/lab/\",\n",
    "        window_size=window_size,\n",
    "        batch_size=batch_size,\n",
    "        config = config,\n",
    "        step_size=step_size,\n",
    "        data_parts= data_parts ,\n",
    "        early_stopping_patience= early_stopping_patience,\n",
    "        epochs= epochs,\n",
    "        model= model,\n",
    "        processor = processor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
