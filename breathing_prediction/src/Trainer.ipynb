{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb70271-70af-41ec-9bc7-b5e596e53424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:43:13.847513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 17:43:13.864350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 17:43:13.869410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 17:43:13.881348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 17:43:14.807341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data : (17, 53, 512000), labels: (17, 53, 800), names : (17,)\n",
      "data : (16, 53, 512000), labels: (16, 53, 800), names : (16,)\n",
      "6\n",
      "data : (6, 53, 512000), labels: (6, 53, 800), names : (6,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glenn/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Wav2Vec2ConvLSTMModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glenn/anaconda3/lib/python3.12/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/200:   0%|          | 0/1431 [00:00<?, ?it/s]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glenn/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training Epoch 1/200, Avg Loss: 0.9964, Acc: 0.0036:   0%|          | 1/1431 [00:03<1:27:06,  3.66s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9922, Acc: 0.0078:   0%|          | 2/1431 [00:06<1:22:39,  3.47s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0050, Acc: -0.0050:   0%|          | 3/1431 [00:10<1:21:27,  3.42s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9886, Acc: 0.0114:   0%|          | 4/1431 [00:13<1:20:43,  3.39s/it] It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9945, Acc: 0.0055:   0%|          | 5/1431 [00:17<1:20:18,  3.38s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9979, Acc: 0.0021:   0%|          | 6/1431 [00:20<1:20:01,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9933, Acc: 0.0067:   0%|          | 7/1431 [00:23<1:19:51,  3.36s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9904, Acc: 0.0096:   1%|          | 8/1431 [00:27<1:19:49,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9969, Acc: 0.0031:   1%|          | 9/1431 [00:30<1:19:53,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9981, Acc: 0.0019:   1%|          | 10/1431 [00:33<1:19:52,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 0.9993, Acc: 0.0007:   1%|          | 11/1431 [00:37<1:19:47,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0056, Acc: -0.0056:   1%|          | 12/1431 [00:40<1:19:40,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0043, Acc: -0.0043:   1%|          | 13/1431 [00:43<1:19:28,  3.36s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0033, Acc: -0.0033:   1%|          | 14/1431 [00:47<1:19:21,  3.36s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0056, Acc: -0.0056:   1%|          | 15/1431 [00:50<1:19:14,  3.36s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0046, Acc: -0.0046:   1%|          | 16/1431 [00:54<1:19:11,  3.36s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0071, Acc: -0.0071:   1%|          | 17/1431 [00:57<1:19:03,  3.35s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0091, Acc: -0.0091:   1%|▏         | 18/1431 [01:00<1:18:57,  3.35s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0112, Acc: -0.0112:   1%|▏         | 19/1431 [01:04<1:18:56,  3.35s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0116, Acc: -0.0116:   1%|▏         | 20/1431 [01:07<1:18:57,  3.36s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0097, Acc: -0.0097:   1%|▏         | 21/1431 [01:10<1:19:05,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/200, Avg Loss: 1.0102, Acc: -0.0102:   2%|▏         | 22/1431 [01:14<1:19:08,  3.37s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512000])\n",
      "Shape after wav2vec2: torch.Size([1, 1599, 768])\n",
      "Shape after permute: torch.Size([1, 768, 1599])\n",
      "Shape after conv: torch.Size([1, 768, 1599])\n",
      "Shape after relu: torch.Size([1, 768, 1599])\n",
      "Shape after second permute: torch.Size([1, 1599, 768])\n",
      "Shape after lstm: torch.Size([1, 1599, 128])\n",
      "Shape after selecting last time step: torch.Size([1, 128])\n",
      "Shape after embedding: torch.Size([1, 128])\n",
      "Shape after output: torch.Size([1, 800])\n",
      "Shape after tanh: torch.Size([1, 800])\n",
      "Shape after flatten: torch.Size([1, 800])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from scipy.io import wavfile\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    Wav2Vec2PreTrainedModel, \n",
    "    Wav2Vec2Config, \n",
    "    WavLMModel, \n",
    "    WavLMConfig, \n",
    "    HubertModel, \n",
    "    HubertConfig, \n",
    "    HubertPreTrainedModel\n",
    ")\n",
    "## local inports \n",
    "from models import *\n",
    "from utils import *\n",
    "from losses import *\n",
    "from dataset import *\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config, model_classes, criterion, device, log_dir, model_save_dir, bert_config):\n",
    "        self.bert_config = bert_config\n",
    "        self.config = config\n",
    "        self.model_classes = model_classes\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.model_save_dir = model_save_dir\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "    def train(self, train_data, val_data, test_data, epochs, batch_size, patience):\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for model_name, model_class in self.model_classes.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            model_config = self.config[model_name]\n",
    "            model_config['output_size'] = train_data.get_output_shape()\n",
    "            model = model_class(bert_config = self.bert_config,config = model_config).to(self.device)\n",
    "            \n",
    "            optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-5)\n",
    "            early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                train_loss, train_acc = self._train_epoch(model, train_loader, optimizer, scheduler, epoch, epochs)\n",
    "                val_loss, val_acc, val_acc_flat = self._evaluate(model, val_loader, val_data)\n",
    "                test_loss, test_acc, test_acc_flat = self._evaluate(model, test_loader, test_data)\n",
    "\n",
    "                self._log_metrics(model_name, epoch, train_loss, val_loss, test_loss, train_acc, val_acc, test_acc, val_acc_flat, test_acc_flat)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    self._save_model(model, f\"{model_name}_best_model_epoch_{epoch}.pt\")\n",
    "\n",
    "                if early_stopping(val_loss):\n",
    "                    print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            results[model_name] = {\n",
    "                'val_loss': best_val_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_acc_flat': test_acc_flat\n",
    "            }\n",
    "\n",
    "        self.writer.close()\n",
    "        return results\n",
    "\n",
    "    def _train_epoch(self, model, dataloader, optimizer, scheduler, epoch, total_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{total_epochs}\")\n",
    "        \n",
    "        for batch_idx, (input_values, labels, _) in enumerate(progress_bar):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_values = self.processor(input_values, return_tensors=\"pt\", padding=\"longest\", sampling_rate = 16000).input_values\n",
    "            input_values = input_values.reshape(input_values.shape[0], input_values.shape[-1])\n",
    "            print(input_values.shape)\n",
    "            input_values, labels = input_values.to(self.device), labels.to(self.device)\n",
    "            predictions = model(input_values)\n",
    "            loss = self.criterion(predictions.float(), labels.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + batch_idx / len(dataloader))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += 1.0 - loss.item()  # Assuming accuracy is 1 - loss for this task\n",
    "            \n",
    "            progress_bar.set_description(f\"Training Epoch {epoch+1}/{total_epochs}, Avg Loss: {total_loss/(batch_idx+1):.4f}, Acc: {total_acc/(batch_idx+1):.4f}\")\n",
    "            \n",
    "            del input_values, labels, predictions, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "    def _evaluate(self, model, dataloader, dataset):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        total_acc_flat = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_values, labels, ground_truth_names in dataloader:\n",
    "                input_values = input_values.to(self.device)\n",
    "                \n",
    "                ground_truth_labels = self._get_ground_truth_labels(ground_truth_names, dataset)\n",
    "                \n",
    "                predictions = self._process_sequences(model, input_values)\n",
    "                loss = self.criterion(predictions.to(\"cpu\"), labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_acc += 1.0 - loss.item() \n",
    "                \n",
    "                average = self._unsplit_data_ogsize(predictions.cpu().numpy(), dataset.window_size, dataset.step_size, dataset.data_points_per_second, ground_truth_labels.shape[-1])\n",
    "                total_acc_flat += self._calculate_flattened_accuracy(average, ground_truth_labels)\n",
    "                \n",
    "                del input_values, labels, predictions, loss\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        num_samples = len(dataloader.dataset)\n",
    "        return total_loss / num_samples, total_acc / num_samples, total_acc_flat / num_samples\n",
    "\n",
    "    def _get_ground_truth_labels(self, ground_truth_names, dataset):\n",
    "        ground_truth_labels = []\n",
    "        for batch_name in ground_truth_names:\n",
    "            ground_truth_label = dataset.choose_real_labs_only_with_filenames([batch_name])\n",
    "            ground_truth_labels.append(ground_truth_label)\n",
    "        return np.array(ground_truth_labels)[:, :, -1].astype(np.float32)\n",
    "\n",
    "    def _process_sequences(self, model, input_values):\n",
    "        predictions = []\n",
    "        for i in range(input_values.size(1)):\n",
    "            input_slice = input_values[:, i, :]\n",
    "            pred = model(input_slice.float())\n",
    "            predictions.append(pred)\n",
    "        return torch.stack(predictions, dim=1)\n",
    "\n",
    "    def _calculate_flattened_accuracy(self, average, ground_truth_labels):\n",
    "        s_acc = 0\n",
    "        for b in range(len(ground_truth_labels)):\n",
    "            s, _ = scipy.stats.pearsonr(average[b], ground_truth_labels[b])\n",
    "            s_acc += s\n",
    "        return s_acc / len(ground_truth_labels)\n",
    "\n",
    "    def _unsplit_data_ogsize(windowed_data, window_size, step_size, data_points_per_second, original_length):\n",
    "        batch_size, num_windows, prediction_size = windowed_data.shape\n",
    "        window_size_points = window_size * data_points_per_second\n",
    "        step_size_points = step_size * data_points_per_second\n",
    "        original_data = np.zeros((batch_size, original_length))\n",
    "        overlap_count = np.zeros((batch_size, original_length))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for i in range(num_windows):\n",
    "                start = i * step_size_points\n",
    "                end = start + window_size_points\n",
    "                if end > original_length:\n",
    "                    end = original_length\n",
    "                segment_length = end - start\n",
    "                original_data[b, start:end] += windowed_data[b, i, :segment_length]\n",
    "                overlap_count[b, start:end] += 1\n",
    "        \n",
    "        # Average the overlapping regions\n",
    "        original_data = np.divide(original_data, overlap_count, where=overlap_count != 0)\n",
    "        \n",
    "        # Trim the data to match the original length\n",
    "        original_data = original_data[:, :original_length]\n",
    "        \n",
    "        return original_data\n",
    "    \n",
    "    def _log_metrics(self, model_name, epoch, train_loss, val_loss, test_loss, train_acc, val_acc, test_acc, val_acc_flat, test_acc_flat):\n",
    "        self.writer.add_scalar(f'{model_name}/Loss/train', train_loss, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Loss/val', val_loss, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Loss/test', test_loss, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Accuracy/train', train_acc, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Accuracy/val', val_acc, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Accuracy/test', test_acc, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Accuracy/val_flat', val_acc_flat, epoch)\n",
    "        self.writer.add_scalar(f'{model_name}/Accuracy/test_flat', test_acc_flat, epoch)\n",
    "\n",
    "    def _save_model(self, model, filename):\n",
    "        path = os.path.join(self.model_save_dir, filename)\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Saved model to {path}\")\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode='min', delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.mode == 'min':\n",
    "            score = -score\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "def prepare_data_model(audio_interspeech_norm, breath_interspeech_folder, window_size, step_size, fold):\n",
    "    # Load and prepare data\n",
    "    train_data, train_labels, train_dict, frame_rate = load_data(audio_interspeech_norm, breath_interspeech_folder, 'train')\n",
    "    devel_data, devel_labels, devel_dict, _ = load_data(audio_interspeech_norm, breath_interspeech_folder, 'devel')\n",
    "    test_data, test_labels, test_dict, _ = load_data(audio_interspeech_norm, breath_interspeech_folder, 'test')\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_train_data, prepared_train_labels, _ = prepare_data(train_data, train_labels, train_dict, frame_rate, window_size * 16000, step_size * 16000)\n",
    "    prepared_devel_data, prepared_devel_labels, _ = prepare_data(devel_data, devel_labels, devel_dict, frame_rate, window_size * 16000, step_size * 16000)\n",
    "    prepared_test_data, prepared_test_labels, _= prepare_data(test_data, test_labels, test_dict, frame_rate, window_size * 16000, step_size * 16000)\n",
    "\n",
    "    # Create custom datasets\n",
    "    train_dataset = CustomDataset(prepared_train_data, prepared_train_labels, train_dict)\n",
    "    val_dataset = CustomDataset(prepared_devel_data, prepared_devel_labels, devel_dict)\n",
    "    test_dataset = CustomDataset(prepared_test_data, prepared_test_labels, test_dict)\n",
    "    train_dataset.print_shapes()\n",
    "    val_dataset.print_shapes()\n",
    "    number_of_test_samples = int((len(val_dataset) + len(train_dataset))/ fold)\n",
    "    print(number_of_test_samples)\n",
    "    new_val_dataset_item = val_dataset.pop_first_n(number_of_test_samples)\n",
    "    new_val_dataset = CustomDataset(new_val_dataset_item[0], new_val_dataset_item[1], new_val_dataset_item[2])\n",
    "    new_val_dataset.print_shapes()\n",
    "\n",
    "    combined_train_data = np.concatenate((train_dataset.data, val_dataset.data), axis=0)\n",
    "    combined_train_labels = np.concatenate((train_dataset.labels, val_dataset.labels), axis=0)\n",
    "    combined_train_dict = np.concatenate((train_dataset.name, val_dataset.name), axis=0)\n",
    "    combined_train_data, combined_train_labels = flatten_data_for_model(combined_train_data, combined_train_labels)\n",
    "    combined_train_dataset = CustomDataset(combined_train_data, combined_train_labels, [])\n",
    "\n",
    "    return combined_train_dataset, new_val_dataset, test_dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_config = HubertConfig.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "\n",
    "    \n",
    "    config = {\n",
    "        \"VRBModel\": {\n",
    "            \"model_name\": \"facebook/hubert-large-ls960\",\n",
    "            \"hidden_units\": 64,\n",
    "            \"n_gru\": 3,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "        \"Wav2Vec2ConvLSTMModel\": {\n",
    "            \"model_name\": \"facebook/wav2vec2-base\",\n",
    "            \"hidden_units\": 128,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output_size\": None \n",
    "        },\n",
    "        \"RespBertLSTMModel\": {\n",
    "            \"bert_config\": \"wav2vec2\",\n",
    "            \"hidden_units\": 128,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output\": None  \n",
    "        },\n",
    "        \"RespBertAttionModel\": {\n",
    "            \"bert_config\": \"hubert\",\n",
    "            \"hidden_units\": 128,\n",
    "            \"n_attion\": 1,\n",
    "            \"output\": None  \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = PearsonLoss()\n",
    "    \n",
    "    model_classes = {\n",
    "        \"Wav2Vec2ConvLSTMModel\": Wav2Vec2ConvLSTMModel,\n",
    "        \"VRBModel\": VRBModel,\n",
    "        \"RespBertLSTMModel\": RespBertLSTMModel,\n",
    "        \"RespBertAttionModel\": RespBertAttionModel,\n",
    "    }\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data, val_data, test_data = prepare_data_model(\n",
    "        \"/home/glenn/Downloads/ComParE2020_Breathing/wav/\",\n",
    "        \"/home/glenn/Downloads/ComParE2020_Breathing/lab/\",\n",
    "        window_size=32,\n",
    "        step_size=4,\n",
    "        fold=5\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(config, model_classes, criterion, device, \"logs\", \"models\", bert_config)\n",
    "\n",
    "    # Train all models\n",
    "    results = trainer.train(train_data, val_data, test_data, epochs=200, batch_size=1, patience=50)\n",
    "\n",
    "    # Print results\n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        for metric, value in model_results.items():\n",
    "            print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cefc04-56a7-46ab-b11f-f7bab4479dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1050 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8ed24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
