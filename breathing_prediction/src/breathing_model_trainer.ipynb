{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1832f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb70271-70af-41ec-9bc7-b5e596e53424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training RespBertLSTMModel...\n",
      "Fold 7/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home3/gdwildt/Master_thesis/breathing_prediction/src/trainer.py:66: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLMWithFlashAttention2(\n",
      "  (feature_extractor): WavLMFeatureEncoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): WavLMLayerNormConvLayer(\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (1-4): 4 x WavLMLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (5-6): 2 x WavLMLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_projection): WavLMFeatureProjection(\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): WavLMEncoderStableLayerNorm(\n",
      "    (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
      "      (conv): ParametrizedConv1d(\n",
      "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "        (parametrizations): ModuleDict(\n",
      "          (weight): ParametrizationList(\n",
      "            (0): _WeightNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (padding): WavLMSamePadLayer()\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): WavLMEncoderLayerStableLayerNorm(\n",
      "        (attention): WavLMAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
      "          (rel_attn_embed): Embedding(320, 16)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): WavLMFeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1-23): 23 x WavLMEncoderLayerStableLayerNorm(\n",
      "        (attention): WavLMAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): WavLMFeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/60:   0%|          | 0/63 [00:00<?, ?it/s]/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Training Epoch 1/60:   0%|          | 0/63 [00:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 39.50 GiB of which 764.12 MiB is free. Including non-PyTorch memory, this process has 38.74 GiB memory in use. Of the allocated memory 36.79 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(device)\n\u001b[1;32m     55\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(config, model_classes, criterion,\n\u001b[1;32m     56\u001b[0m                   device, bert_config, ground_labels, processor)\n\u001b[0;32m---> 57\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(train_data, test_data)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/Master_thesis/breathing_prediction/src/trainer.py:153\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_data, test_data)\u001b[0m\n\u001b[1;32m    148\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpatience, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m     \u001b[39m#val_loss, val_acc , val_flat_acc, val_flat_acc_ola = self._evaluate(model, val_loader)\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(model, train_loader, optimizer,scheduler, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mepochs)\n\u001b[1;32m    154\u001b[0m     val_loss, val_acc , val_flat_acc, val_flat_acc_ola \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate(model, val_loader)\n\u001b[1;32m    155\u001b[0m     test_loss, test_acc, test_flat_acc,  test_flat_acc_ola \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate(model, test_loader)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/Master_thesis/breathing_prediction/src/trainer.py:215\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, model, dataloader, optimizer, scheduler, epoch, total_epochs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     input_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor(input_values, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m\"\u001b[39m, sampling_rate \u001b[39m=\u001b[39m \u001b[39m16000\u001b[39m)\n\u001b[1;32m    214\u001b[0m     input_values, labels \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), labels\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \n\u001b[0;32m--> 215\u001b[0m     predictions \u001b[39m=\u001b[39m model(input_values)\n\u001b[1;32m    216\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(predictions, labels)\n\u001b[1;32m    218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()  \u001b[39m# Scale the loss and call backward\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/Master_thesis/breathing_prediction/src/models.py:372\u001b[0m, in \u001b[0;36mRespBertLSTMModel_flash.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_values):\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m     \u001b[39m#input_values[\"input_values\"] = input_values[\"input_values\"]\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[39m#input_values = input_values\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_values)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    373\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n\u001b[1;32m    374\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py:1227\u001b[0m, in \u001b[0;36mWavLMModel.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m hidden_states, extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1223\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1224\u001b[0m     hidden_states, mask_time_indices\u001b[39m=\u001b[39mmask_time_indices, attention_mask\u001b[39m=\u001b[39mattention_mask\n\u001b[1;32m   1225\u001b[0m )\n\u001b[0;32m-> 1227\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1228\u001b[0m     hidden_states,\n\u001b[1;32m   1229\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1230\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1231\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1232\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1233\u001b[0m )\n\u001b[1;32m   1235\u001b[0m hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py:807\u001b[0m, in \u001b[0;36mWavLMEncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    799\u001b[0m         layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    800\u001b[0m             layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    801\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    804\u001b[0m             output_attentions,\n\u001b[1;32m    805\u001b[0m         )\n\u001b[1;32m    806\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m         layer_outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    808\u001b[0m             hidden_states,\n\u001b[1;32m    809\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    810\u001b[0m             output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    811\u001b[0m             position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    812\u001b[0m         )\n\u001b[1;32m    813\u001b[0m     hidden_states, position_bias \u001b[39m=\u001b[39m layer_outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    815\u001b[0m \u001b[39mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py:650\u001b[0m, in \u001b[0;36mWavLMEncoderLayerStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, output_attentions)\u001b[0m\n\u001b[1;32m    648\u001b[0m attn_residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    649\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 650\u001b[0m hidden_states, attn_weights, position_bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    651\u001b[0m     hidden_states,\n\u001b[1;32m    652\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    653\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    654\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    655\u001b[0m )\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    657\u001b[0m hidden_states \u001b[39m=\u001b[39m attn_residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py:473\u001b[0m, in \u001b[0;36mWavLMAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, output_attentions, index)\u001b[0m\n\u001b[1;32m    470\u001b[0m gated_position_bias \u001b[39m=\u001b[39m gate_output\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m position_bias\n\u001b[1;32m    471\u001b[0m gated_position_bias \u001b[39m=\u001b[39m gated_position_bias\u001b[39m.\u001b[39mview((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, tgt_len, tgt_len))\n\u001b[0;32m--> 473\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_multi_head_self_attention(\n\u001b[1;32m    474\u001b[0m     hidden_states, attention_mask, gated_position_bias, output_attentions\n\u001b[1;32m    475\u001b[0m )\n\u001b[1;32m    477\u001b[0m \u001b[39mreturn\u001b[39;00m attn_output, attn_weights, position_bias\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/transformers/models/wavlm/modeling_wavlm.py:497\u001b[0m, in \u001b[0;36mWavLMAttention.torch_multi_head_self_attention\u001b[0;34m(self, hidden_states, attention_mask, gated_position_bias, output_attentions)\u001b[0m\n\u001b[1;32m    493\u001b[0m add_zero_attn \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39m# PyTorch 1.3.0 has F.multi_head_attention_forward defined\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m# so no problem with backwards compatibility\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m    498\u001b[0m     query,\n\u001b[1;32m    499\u001b[0m     key,\n\u001b[1;32m    500\u001b[0m     value,\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim,\n\u001b[1;32m    502\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m    503\u001b[0m     torch\u001b[39m.\u001b[39;49mempty([\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m    504\u001b[0m     torch\u001b[39m.\u001b[39;49mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_proj\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj\u001b[39m.\u001b[39;49mbias)),\n\u001b[1;32m    505\u001b[0m     bias_k,\n\u001b[1;32m    506\u001b[0m     bias_v,\n\u001b[1;32m    507\u001b[0m     add_zero_attn,\n\u001b[1;32m    508\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout,\n\u001b[1;32m    509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m    512\u001b[0m     key_padding_mask,\n\u001b[1;32m    513\u001b[0m     output_attentions,\n\u001b[1;32m    514\u001b[0m     gated_position_bias,\n\u001b[1;32m    515\u001b[0m     use_separate_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    516\u001b[0m     q_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    517\u001b[0m     k_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    518\u001b[0m     v_proj_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    519\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[39m# [Seq_Len, Batch Size, ...] -> [Batch Size, Seq_Len, ...]\u001b[39;00m\n\u001b[1;32m    522\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/home3/gdwildt/micromamba/envs/mamba_thesis/lib/python3.11/site-packages/torch/nn/functional.py:5560\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5557\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5558\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5560\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5561\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5563\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 39.50 GiB of which 764.12 MiB is free. Including non-PyTorch memory, this process has 38.74 GiB memory in use. Of the allocated memory 36.79 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Local imports\n",
    "from models import *\n",
    "from utils import *\n",
    "from losses import *\n",
    "from dataset import *\n",
    "from trainer import *\n",
    "from config import Config\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "    config = Config()\n",
    "    # Define the model name\n",
    "\n",
    "    # Load the processor\n",
    "\n",
    "    # bert_config = HubertConfig.from_pretrained(config.bert_model)attaion_model_20_epochs_85\n",
    "\n",
    "    # Enable Flash Attention\n",
    "\n",
    "    model_classes = {\n",
    "        # \"VRBModel\": VRBModel,\n",
    "        # \"wav2vec2_1DCNN\" : Model1DCNN,\n",
    "        # \"OG_1DCNN\" : Model1DCNN,\n",
    "        \"RespBertLSTMModel\": RespBertLSTMModel_flash,\n",
    "        # \"Wav2Vec2ConvLSTMModel\": Wav2Vec2ConvLSTMModel,\n",
    "        # \"RespBertAttionModel\": RespBertAttionModel,\n",
    "    }\n",
    "\n",
    "    # Prepare data\n",
    "    train_data, test_data, ground_labels = prepare_data_model(\n",
    "        config.audio_interspeech_norm,\n",
    "        config.breath_interspeech_folder,\n",
    "        window_size=config.window_size,\n",
    "        step_size=config.step_size,\n",
    "    )\n",
    "    model_name = \"microsoft/wavlm-large\"\n",
    "    #bert_config = AutoConfig.from_pretrained(model_name)\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        \"microsoft/wavlm-large\", force_download=True)\n",
    "    #model_name = \"patrickvonplaten/wavlm-libri-clean-100h-base-plus\"\n",
    "    #model_name = \"microsoft/wavlm-large\"\n",
    "    #bert_config = AutoConfig.from_pretrained(model_name)\n",
    "    #processor = AutoProcessor.from_pretrained(model_name)\n",
    "    bert_config = AutoConfig.from_pretrained(model_name)\n",
    "    #processor = AutoProcessor.from_pretrained(model_name)\n",
    "    device = torch.device(\n",
    "        config.device if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = PearsonLoss()\n",
    "    print(device)\n",
    "\n",
    "    trainer = Trainer(config, model_classes, criterion,\n",
    "                      device, bert_config, ground_labels, processor)\n",
    "    trainer.train(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cefc04-56a7-46ab-b11f-f7bab4479dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(torch.cuda.get_device_name())\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8ed24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e2b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
