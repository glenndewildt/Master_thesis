{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pt_utils import *\n",
    "from pt_dataset import *\n",
    "from pt_models import *\n",
    "from pt_utils import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from transformers import Wav2Vec2Processor,Wav2Vec2FeatureExtractor,AutoModel\n",
    "from tensorboardX import SummaryWriter\n",
    "from pt_utils import load_data, prepare_data, reshaping_data_for_model, unsplit_data_ogsize\n",
    "from pt_dataset import BreathingDataset\n",
    "import scipy.stats\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def create_run_directory():\n",
    "    base_dir = \"pt_runs\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(base_dir, timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def _calculate_flattened_accuracy(average, ground_truth_labels):\n",
    "    s_acc = 0\n",
    "    for b in range(len(ground_truth_labels)):\n",
    "        s, _ = scipy.stats.pearsonr(average[b], ground_truth_labels[b])\n",
    "        s_acc += s\n",
    "    return s_acc / len(ground_truth_labels)\n",
    "\n",
    "def _choose_real_labs_only_with_filenames(labels, filenames):\n",
    "    return labels[labels['filename'].isin(filenames)]\n",
    "\n",
    "def _get_ground_truth_labels(ground_truth_names, labels):\n",
    "    ground_truth_labels = []\n",
    "    for batch_name in ground_truth_names:\n",
    "        ground_truth_label = _choose_real_labs_only_with_filenames(labels, [batch_name])\n",
    "        ground_truth_labels.append(ground_truth_label)\n",
    "    return np.array(ground_truth_labels)[:, :, -1].astype(np.float32)\n",
    "\n",
    "def prepare_test_datasets(path_to_test_data, path_to_test_labels, window_size=16, step_size=6, batch_size=10, processor=None):\n",
    "    \"\"\"\n",
    "    Load and prepare test datasets, saving them for later use\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    length_sequence = window_size \n",
    "    step_sequence = step_size\n",
    "\n",
    "    # Load and prepare test data\n",
    "    test_data, test_labels, test_dict, frame_rate = load_data(path_to_test_data, path_to_test_labels, 'test')\n",
    "    prepared_test_data, prepared_test_labels, prepared_test_labels_timesteps = prepare_data(\n",
    "        test_data, test_labels, test_dict, frame_rate, \n",
    "        length_sequence * 16000, step_sequence * 16000\n",
    "    )\n",
    "\n",
    "    # Reshape data\n",
    "    test_d, test_lbs = reshaping_data_for_model(prepared_test_data, prepared_test_labels)\n",
    "    print(f\"Test data shape: {test_d.shape}\")\n",
    "\n",
    "    # Create dataset\n",
    "    test_dataset = BreathingDataset(test_d, test_lbs, processor, window_size, step_sequence)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=3, collate_fn=test_dataset.collate_fn)\n",
    "    \n",
    "    # Save the prepared data\n",
    "    save_path = f'prepared_test_data_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.npz'\n",
    "    np.savez_compressed(save_path, \n",
    "                       test_labels=test_labels,\n",
    "                       test_dict=test_dict,\n",
    "                       prepared_test_labels_timesteps=prepared_test_labels_timesteps,\n",
    "                       output_size=prepared_test_labels.shape[-1])\n",
    "    \n",
    "    print(f\"Saved prepared data to {save_path}\")\n",
    "    \n",
    "    return test_loader, save_path\n",
    "\n",
    "def run_model_inference(test_loader, prepared_data_file, model_path=None, config=None):\n",
    "    \"\"\"\n",
    "    Run model inference using prepared data loader\n",
    "    \"\"\"\n",
    "    # Load prepared data info\n",
    "    data_info = np.load(prepared_data_file, allow_pickle=True)\n",
    "    config[\"output_size\"] = int(data_info['output_size'])\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    model = config[\"model\"](config)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model = model.half()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_loss = 0.0\n",
    "    progress_bar = tqdm(test_loader, desc=f\"Test\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_d, batch_lbs in progress_bar:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                batch_d = batch_d.to(device)\n",
    "                batch_lbs = batch_lbs.to(device)\n",
    "                batch_d = model(batch_d)\n",
    "                loss = correlation_coefficient_loss(batch_d, batch_lbs)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_pred.extend(batch_d.float().cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'test loss: ': f'{test_loss/(progress_bar.n+1):.4f}'})\n",
    "            \n",
    "            del loss, batch_d, batch_lbs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    # Get shape from saved data\n",
    "    prepared_test_labels_timesteps = data_info['prepared_test_labels_timesteps']\n",
    "    test_pred = np.array(test_pred).reshape(prepared_test_labels_timesteps.shape)\n",
    "    \n",
    "    # Save predictions and necessary data\n",
    "    save_path = f'model_predictions_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.npz'\n",
    "    np.savez_compressed(save_path,\n",
    "                       predictions=test_pred,\n",
    "                       test_labels=data_info['test_labels'],\n",
    "                       test_dict=data_info['test_dict'],\n",
    "                       prepared_test_labels_timesteps=prepared_test_labels_timesteps,\n",
    "                       test_loss=test_loss)\n",
    "    \n",
    "    print(f\"Saved predictions to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "def calculate_metrics(predictions_file, run_dir=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics from saved predictions\n",
    "    \"\"\"\n",
    "    if run_dir is None:\n",
    "        run_dir = create_run_directory()\n",
    "    \n",
    "    log_dir = os.path.join(run_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Load saved predictions and data\n",
    "    data = np.load(predictions_file, allow_pickle=True)\n",
    "    test_pred = data['predictions']\n",
    "    test_labels = data['test_labels']\n",
    "    test_dict = data['test_dict'].item()  # Convert numpy object array to dict\n",
    "    prepared_test_labels_timesteps = data['prepared_test_labels_timesteps']\n",
    "    test_loss = float(data['test_loss'])\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_ground_truth = _get_ground_truth_labels(list(test_dict.values()), test_labels)\n",
    "    test_pred_flat = concatenate_prediction(test_labels, test_pred, prepared_test_labels_timesteps, test_dict)\n",
    "    test_prc_coef = _calculate_flattened_accuracy(test_pred_flat, test_ground_truth)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nEvaluation completed.\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Pearson Coefficient (flattened): {test_prc_coef:.4f}\")\n",
    "\n",
    "    # Log with tensorboard\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    writer.add_scalar(\"Test/loss\", test_loss, 0)\n",
    "    writer.add_scalar(\"Test/pearson_coef\", test_prc_coef, 0)\n",
    "    \n",
    "    test_table = \"| Metric | Value |\\n\" \\\n",
    "                 \"|--------|-------|\\n\" \\\n",
    "                 f\"| Test Loss | {test_loss:.4f} |\\n\" \\\n",
    "                 f\"| Test Pearson Coefficient | {test_prc_coef:.4f} |\\n\"\n",
    "    writer.add_text(\"Test_Metrics\", test_table)\n",
    "    writer.close()\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'Test_Loss': [test_loss],\n",
    "        'Test_Pearson_Coefficient': [test_prc_coef]\n",
    "    })\n",
    "    csv_path = os.path.join(run_dir, 'test_results.csv')\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (160, 480000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_186276/2366093783.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "Test:   0%|          | 0/40 [00:00<?, ?it/s]/home/glenn/mambaforge/envs/mamba/lib/python3.11/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Test: 100%|██████████| 40/40 [05:47<00:00,  8.68s/it, test loss: =0.1761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions and data to model_predictions_20241023-190407.npz\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    path = \"/home/glenn/Downloads/\"\n",
    "    #path = \"../DATA/\"\n",
    "\n",
    "\n",
    "    # Model parameters\n",
    "    model_config = {\n",
    "        \"RespBertCNNModel\": {\n",
    "            'model' : RespBertCNNModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"output_size\": None  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Evaluation parameters\n",
    "    window_size = 30\n",
    "    step_size = 25\n",
    "    batch_size = 4\n",
    "    \n",
    "    config = model_config[\"RespBertCNNModel\"]\n",
    "    #processor = Wav2vec2F.from_pretrained(config[\"model_name\"])\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "    # Create and initialize model\n",
    "    model_folder = \"/home/glenn/Downloads/pt_runs/pt_runs/\"\n",
    "\n",
    "    # Load the pre-trained model weights\n",
    "    model_path = model_folder+\"Wavml_cnn_full/best_model\"  # Update this path\n",
    "\n",
    "\n",
    "    test_loader, prepared_data_file = prepare_test_datasets(\n",
    "    path_to_test_data=path+\"ComParE2020_Breathing/wav/\",\n",
    "    path_to_test_labels=path+\"ComParE2020_Breathing/lab/\",\n",
    "    processor=processor\n",
    "    )\n",
    "\n",
    "    # Then run the model inference\n",
    "    predictions_file = run_model_inference(\n",
    "        test_loader=test_loader,\n",
    "        prepared_data_file=prepared_data_file,\n",
    "        model_path='path/to/model',\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # Later, calculate metrics from saved predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (624, 256000)\n",
      "Saved prepared data to prepared_test_data_20241023-191657.npz\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#path = \"../DATA/\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_loader, prepared_data_file \u001b[38;5;241m=\u001b[39m prepare_test_datasets(\n\u001b[1;32m      6\u001b[0m path_to_test_data\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComParE2020_Breathing/wav/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m path_to_test_labels\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComParE2020_Breathing/lab/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m processor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 152\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(predictions_file, run_dir)\u001b[0m\n\u001b[1;32m    149\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m test_ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43m_get_ground_truth_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m test_pred_flat \u001b[38;5;241m=\u001b[39m concatenate_prediction(test_labels, test_pred, prepared_test_labels_timesteps, test_dict)\n\u001b[1;32m    154\u001b[0m test_prc_coef \u001b[38;5;241m=\u001b[39m _calculate_flattened_accuracy(test_pred_flat, test_ground_truth)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36m_get_ground_truth_labels\u001b[0;34m(ground_truth_names, labels)\u001b[0m\n\u001b[1;32m     36\u001b[0m ground_truth_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_name \u001b[38;5;129;01min\u001b[39;00m ground_truth_names:\n\u001b[0;32m---> 38\u001b[0m     ground_truth_label \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_real_labs_only_with_filenames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     ground_truth_labels\u001b[38;5;241m.\u001b[39mappend(ground_truth_label)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ground_truth_labels)[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36m_choose_real_labs_only_with_filenames\u001b[0;34m(labels, filenames)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_choose_real_labs_only_with_filenames\u001b[39m(labels, filenames):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels[\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin(filenames)]\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "predictions_file = \"/home/glenn/Documents/GitHub/Master_thesis/1dcnn_breathing_prediction/model_predictions_20241023-190407.npz\"\n",
    "path = \"/home/glenn/Downloads/\"\n",
    "#path = \"../DATA/\"\n",
    "\n",
    "test_loader, prepared_data_file = prepare_test_datasets(\n",
    "path_to_test_data=path+\"ComParE2020_Breathing/wav/\",\n",
    "path_to_test_labels=path+\"ComParE2020_Breathing/lab/\",\n",
    "processor=None\n",
    ")\n",
    "\n",
    "calculate_metrics(predictions_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
