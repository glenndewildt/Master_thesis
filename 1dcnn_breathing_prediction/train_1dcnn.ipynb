{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mImportError: /home/glenn/micromamba/envs/mthesis/lib/python3.12/lib-dynload/_sqlite3.cpython-312-x86_64-linux-gnu.so: undefined symbol: sqlite3_deserialize. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from datetime import datetime\n",
    "from utils_glenn import *  # Assuming this file contains necessary utility functions\n",
    "\n",
    "def create_run_directory():\n",
    "    base_dir = \"runs\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(base_dir, timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def _calculate_flattened_accuracy(average, ground_truth_labels):\n",
    "    s_acc = 0\n",
    "    for b in range(len(ground_truth_labels)):\n",
    "        s, _ = scipy.stats.pearsonr(average[b], ground_truth_labels[b])\n",
    "        s_acc += s\n",
    "    return s_acc / len(ground_truth_labels)\n",
    "\n",
    "def _choose_real_labs_only_with_filenames(labels, filenames):\n",
    "    return labels[labels['filename'].isin(filenames)]\n",
    "\n",
    "def _get_ground_truth_labels(ground_truth_names, labels):\n",
    "    ground_truth_labels = []\n",
    "    for batch_name in ground_truth_names:\n",
    "        ground_truth_label = _choose_real_labs_only_with_filenames(labels, [batch_name])\n",
    "        ground_truth_labels.append(ground_truth_label)\n",
    "    return np.array(ground_truth_labels)[:, :, -1].astype(np.float32)\n",
    "\n",
    "class CustomTensorBoardCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir, fold):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.fold = fold\n",
    "        self.writer = tf.summary.create_file_writer(os.path.join(log_dir, f\"fold_{fold}\"))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with self.writer.as_default():\n",
    "            for name, value in logs.items():\n",
    "                tf.summary.scalar(f\"{name}_fold_{self.fold}\", value, step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "def train(path_to_data, path_to_labels, window_size = 16, step_size =2/5, data_parts=4, epochs = 100, batch_size =10, early_stopping_patience = 20):\n",
    "    run_dir = create_run_directory()\n",
    "    log_dir = os.path.join(run_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Parameters\n",
    "    length_sequence = window_size \n",
    "    step_sequence = int(step_size* window_size)\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_data, train_labels, train_dict, frame_rate = load_data(path_to_data, path_to_labels, 'train')\n",
    "    devel_data, devel_labels, devel_dict, frame_rate = load_data(path_to_data, path_to_labels, 'devel')\n",
    "    test_data, test_labels, test_dict, frame_rate = load_data(path_to_data, path_to_labels, 'test')\n",
    "\n",
    "    # Combine train and devel data\n",
    "    all_data = np.concatenate((train_data, devel_data), axis=0)\n",
    "    all_labels = pd.concat([train_labels, devel_labels])\n",
    "    all_dict = np.concatenate((list(train_dict.values()), list(devel_dict.values())), axis=0)\n",
    "    # Prepare data\n",
    "    prepared_data, prepared_labels, prepared_labels_timesteps = prepare_data(all_data, all_labels, all_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "    prepared_test_data, prepared_test_labels, prepared_test_labels_timesteps = prepare_data(test_data, test_labels, test_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "\n",
    "    # Create CSV file for storing fold indices\n",
    "    fold_indices_df = pd.DataFrame(columns=['Fold', 'Train_Indices', 'Val_Indices'])\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = KFold(n_splits=data_parts)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(prepared_data)):\n",
    "        print(f\"Fold {fold + 1}/{data_parts}\")\n",
    "\n",
    "        # Save fold indices\n",
    "        fold_indices_df = fold_indices_df._append({\n",
    "            'Fold': fold + 1,\n",
    "            'Train_Indices': train_index.tolist(),\n",
    "            'Val_Indices': val_index.tolist()\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Split data\n",
    "        train_d, val_d = prepared_data[train_index], prepared_data[val_index]\n",
    "        train_lbs, val_lbs = prepared_labels[train_index], prepared_labels[val_index]\n",
    "        train_timesteps, val_timesteps = prepared_labels_timesteps[train_index], prepared_labels_timesteps[val_index]\n",
    "\n",
    "        # Reshape data\n",
    "        train_d, train_lbs = reshaping_data_for_model(train_d, train_lbs)\n",
    "        val_d, val_lbs = reshaping_data_for_model(val_d, val_lbs)\n",
    "        test_d, test_lbs = reshaping_data_for_model(prepared_test_data, prepared_test_labels)\n",
    "\n",
    "        # Create and compile model\n",
    "        model = create_1dcnn(input_shape=(train_d.shape[-2], train_d.shape[-1]))\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer, loss=correlation_coefficient_loss,\n",
    "                      metrics=['mse', 'mae', correlation_coefficient_accuracy])\n",
    "\n",
    "        # Callbacks\n",
    "        tb_callback = CustomTensorBoardCallback(log_dir, fold + 1)\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_d, train_lbs, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=(val_d, val_lbs), \n",
    "                            callbacks=[tb_callback, early_stopping, reduce_lr])\n",
    "\n",
    "        # Evaluate model on validation data\n",
    "        \n",
    "        val_pred = model.predict(val_d, batch_size=batch_size)\n",
    "        val_pred = val_pred.reshape(val_timesteps.shape)\n",
    "        val_ground_truth = _get_ground_truth_labels([all_dict[i] for i in val_index], all_labels)\n",
    "        val_pred_flat = unsplit_data_ogsize(val_pred, window_size, step_sequence, 25, val_ground_truth.shape[-1])\n",
    "        val_prc_coef = _calculate_flattened_accuracy(val_pred_flat, val_ground_truth)\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        test_pred = model.predict(test_d, batch_size=batch_size)\n",
    "        test_pred = test_pred.reshape(prepared_test_labels_timesteps.shape)\n",
    "        test_ground_truth = _get_ground_truth_labels(list(test_dict.values()), test_labels)\n",
    "        test_pred_flat = unsplit_data_ogsize(test_pred, window_size, step_sequence, 25, test_ground_truth.shape[-1])\n",
    "        test_prc_coef = _calculate_flattened_accuracy(test_pred_flat, test_ground_truth)\n",
    "        test_loss, test_mse, test_mae, test_acc = model.evaluate(test_d, test_lbs, batch_size=batch_size)\n",
    "\n",
    "        print(f\"Fold {fold + 1}:\")\n",
    "        print(f\"  Validation Pearson Coefficient: {val_prc_coef}\")\n",
    "        print(f\"  Test Loss: {test_loss}\")\n",
    "        print(f\"  Test MSE: {test_mse}\")\n",
    "        print(f\"  Test MAE: {test_mae}\")\n",
    "        print(f\"  Test Pearson Coefficient: {test_acc}\")\n",
    "        print(f\"  Test Pearson Coefficient (flattened): {test_prc_coef}\")\n",
    "\n",
    "        fold_metrics.append({\n",
    "            'val_prc': val_prc_coef,\n",
    "            'test_loss': test_loss,\n",
    "            'test_mse': test_mse,\n",
    "            'test_mae': test_mae,\n",
    "            'test_prc': test_acc,\n",
    "            'test_prc_flat': test_prc_coef\n",
    "        })\n",
    "\n",
    "        # Save fold model\n",
    "        model.save(os.path.join(run_dir, f'model_fold_{fold + 1}.h5'))\n",
    "\n",
    "        # Log fold-specific metrics\n",
    "        with tf.summary.create_file_writer(log_dir).as_default():\n",
    "            tf.summary.scalar(f\"val_flat_pearson_coefficient_fold_{fold + 1}\", val_prc_coef, step=1)\n",
    "            tf.summary.scalar(f\"test_loss_fold_{fold + 1}\", test_loss, step=1)\n",
    "            tf.summary.scalar(f\"test_mse_fold_{fold + 1}\", test_mse, step=1)\n",
    "            tf.summary.scalar(f\"test_mae_fold_{fold + 1}\", test_mae, step=1)\n",
    "            tf.summary.scalar(f\"test_pearson_coefficient_fold_{fold + 1}\", test_acc, step=1)\n",
    "            tf.summary.scalar(f\"test_pearson_coefficient_flat_fold_{fold + 1}\", test_prc_coef, step=1)\n",
    "\n",
    "    # Calculate and log average metrics\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0].keys()}\n",
    "    \n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        for key, value in avg_metrics.items():\n",
    "            tf.summary.scalar(f\"average_{key}\", value, step=1)\n",
    "\n",
    "    # Save the model with the best average performance\n",
    "    best_fold = np.argmax([fold['test_prc_flat'] for fold in fold_metrics])\n",
    "    best_model_path = os.path.join(run_dir, f'model_fold_{best_fold + 1}.h5')\n",
    "    os.rename(best_model_path, os.path.join(run_dir, 'best_model.h5'))\n",
    "\n",
    "    # Save fold indices CSV\n",
    "    fold_indices_df.to_csv(os.path.join(run_dir, 'fold_indices.csv'), index=False)\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(f\"Best model (Fold {best_fold + 1}) saved.\")\n",
    "    print(\"Average metrics across all folds:\")\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/home/glenn/Downloads/ComParE2020_Breathing\"\n",
    "    train(\n",
    "        path_to_data=path+\"/wav/\",\n",
    "        path_to_labels=path+\"/lab/\",\n",
    "        window_size=16,\n",
    "        batch_size=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
