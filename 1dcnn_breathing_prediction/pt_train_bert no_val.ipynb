{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n",
      "(864, 480000)\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/70:   0%|          | 0/72 [00:00<?, ?it/s]/gpfs/home3/gdwildt/Master_thesis/1dcnn_breathing_prediction/pt_dataset.py:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/gdwildt/.conda/envs/thesis/lib/python3.11/site-packages/torch/nn/functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/70: 100%|██████████| 72/72 [02:27<00:00,  2.05s/it, train_loss=0.9958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70 - Train Loss: 0.9958, Val Loss: 0.9521, Val Pearson: 0.0910\n",
      "Validation loss improved from inf to 0.9521. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/70: 100%|██████████| 72/72 [02:28<00:00,  2.06s/it, train_loss=0.7800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/70 - Train Loss: 0.7800, Val Loss: 0.4329, Val Pearson: 0.6395\n",
      "Validation loss improved from 0.9521 to 0.4329. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/70: 100%|██████████| 72/72 [02:28<00:00,  2.06s/it, train_loss=0.4646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/70 - Train Loss: 0.4646, Val Loss: 0.2692, Val Pearson: 0.7474\n",
      "Validation loss improved from 0.4329 to 0.2692. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/70: 100%|██████████| 72/72 [02:25<00:00,  2.02s/it, train_loss=0.3839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/70 - Train Loss: 0.3839, Val Loss: 0.2548, Val Pearson: 0.7570\n",
      "Validation loss improved from 0.2692 to 0.2548. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/70: 100%|██████████| 72/72 [02:27<00:00,  2.05s/it, train_loss=0.3486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/70 - Train Loss: 0.3486, Val Loss: 0.2013, Val Pearson: 0.7993\n",
      "Validation loss improved from 0.2548 to 0.2013. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/70: 100%|██████████| 72/72 [02:26<00:00,  2.04s/it, train_loss=0.3138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/70 - Train Loss: 0.3138, Val Loss: 0.1922, Val Pearson: 0.8090\n",
      "Validation loss improved from 0.2013 to 0.1922. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/70: 100%|██████████| 72/72 [02:28<00:00,  2.06s/it, train_loss=0.2747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/70 - Train Loss: 0.2747, Val Loss: 0.1749, Val Pearson: 0.8219\n",
      "Validation loss improved from 0.1922 to 0.1749. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/70: 100%|██████████| 72/72 [02:25<00:00,  2.01s/it, train_loss=0.2560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/70 - Train Loss: 0.2560, Val Loss: 0.1694, Val Pearson: 0.8275\n",
      "Validation loss improved from 0.1749 to 0.1694. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/70: 100%|██████████| 72/72 [02:29<00:00,  2.07s/it, train_loss=0.2261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/70 - Train Loss: 0.2261, Val Loss: 0.1730, Val Pearson: 0.8311\n",
      "Validation loss did not improve for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/70: 100%|██████████| 72/72 [02:27<00:00,  2.05s/it, train_loss=0.2344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/70 - Train Loss: 0.2344, Val Loss: 0.1449, Val Pearson: 0.8453\n",
      "Validation loss improved from 0.1694 to 0.1449. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/70: 100%|██████████| 72/72 [02:25<00:00,  2.02s/it, train_loss=0.2161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/70 - Train Loss: 0.2161, Val Loss: 0.1590, Val Pearson: 0.8366\n",
      "Validation loss did not improve for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/70: 100%|██████████| 72/72 [02:26<00:00,  2.04s/it, train_loss=0.1913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/70 - Train Loss: 0.1913, Val Loss: 0.1457, Val Pearson: 0.8494\n",
      "Validation loss did not improve for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/70: 100%|██████████| 72/72 [02:26<00:00,  2.03s/it, train_loss=0.1885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/70 - Train Loss: 0.1885, Val Loss: 0.1441, Val Pearson: 0.8516\n",
      "Validation loss improved from 0.1449 to 0.1441. Saving best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/70: 100%|██████████| 72/72 [02:28<00:00,  2.06s/it, train_loss=0.1898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/70 - Train Loss: 0.1898, Val Loss: 0.1562, Val Pearson: 0.8419\n",
      "Validation loss did not improve for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/70: 100%|██████████| 72/72 [02:29<00:00,  2.07s/it, train_loss=0.1789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/70 - Train Loss: 0.1789, Val Loss: 0.1510, Val Pearson: 0.8424\n",
      "Validation loss did not improve for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/70: 100%|██████████| 72/72 [02:29<00:00,  2.07s/it, train_loss=0.1795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/70 - Train Loss: 0.1795, Val Loss: 0.1505, Val Pearson: 0.8415\n",
      "Validation loss did not improve for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/70: 100%|██████████| 72/72 [02:26<00:00,  2.04s/it, train_loss=0.1777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/70 - Train Loss: 0.1777, Val Loss: 0.1558, Val Pearson: 0.8374\n",
      "Validation loss did not improve for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/70:  19%|█▉        | 14/72 [00:28<02:01,  2.10s/it, train_loss=0.1441]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torchaudio\n",
    "from typing import List, Tuple\n",
    "from pt_utils import *\n",
    "from pt_dataset import *\n",
    "from pt_models import *\n",
    "from pt_utils import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "def create_run_directory():\n",
    "    base_dir = \"pt_runs\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(base_dir, timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def _calculate_flattened_accuracy(average, ground_truth_labels):\n",
    "    s_acc = 0\n",
    "    for b in range(len(ground_truth_labels)):\n",
    "        s, _ = scipy.stats.pearsonr(average[b], ground_truth_labels[b])\n",
    "        s_acc += s\n",
    "    return s_acc / len(ground_truth_labels)\n",
    "\n",
    "def _choose_real_labs_only_with_filenames(labels, filenames):\n",
    "    return labels[labels['filename'].isin(filenames)]\n",
    "\n",
    "def _get_ground_truth_labels(ground_truth_names, labels):\n",
    "    ground_truth_labels = []\n",
    "    for batch_name in ground_truth_names:\n",
    "        ground_truth_label = _choose_real_labs_only_with_filenames(labels, [batch_name])\n",
    "        ground_truth_labels.append(ground_truth_label)\n",
    "    return np.array(ground_truth_labels)[:, :, -1].astype(np.float32)\n",
    "\n",
    "def train(path_to_data, path_to_labels, window_size=16, step_size=6, epochs=100, batch_size=10, config=None, model=None, processor=None):\n",
    "    run_dir = create_run_directory()\n",
    "    log_dir = os.path.join(run_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Parameters\n",
    "    length_sequence = window_size \n",
    "    step_sequence = step_size\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_data, train_labels, train_dict, frame_rate = load_data(path_to_data, path_to_labels, 'train')\n",
    "    devel_data, devel_labels, devel_dict, frame_rate = load_data(path_to_data, path_to_labels, 'devel')\n",
    "    test_data, test_labels, test_dict, frame_rate = load_data(path_to_data, path_to_labels, 'test')\n",
    "\n",
    "    # Combine train and devel data\n",
    "    all_data = np.concatenate((train_data, devel_data), axis=0)\n",
    "    all_labels = pd.concat([train_labels, devel_labels])\n",
    "    all_dict = np.concatenate((list(train_dict.values()), list(devel_dict.values())), axis=0)\n",
    "\n",
    "    # Prepare data\n",
    "    prepared_data, prepared_labels, prepared_labels_timesteps = prepare_data(all_data, all_labels, all_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "    prepared_test_data, prepared_test_labels, prepared_test_labels_timesteps = prepare_data(test_data, test_labels, test_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    config[\"output_size\"] = prepared_labels.shape[-1]\n",
    "    writer = SummaryWriter(log_dir=os.path.join(log_dir, config[\"model_name\"]))\n",
    "\n",
    "    # Reshape data\n",
    "    train_d, train_lbs = reshaping_data_for_model(prepared_data, prepared_labels)\n",
    "    test_d, test_lbs = reshaping_data_for_model(prepared_test_data, prepared_test_labels)\n",
    "    \n",
    "    print(train_d.shape)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = GPUBreathingDataset(train_d, train_lbs, processor, augment=True)\n",
    "    test_dataset = BreathingDataset(test_d, test_lbs, processor, window_size, step_sequence)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=5, collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "    # Create and initialize model\n",
    "    model = config[\"model\"](config).to(device)\n",
    "\n",
    "    # Optimizer and scheduler setup\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 0.001\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                         num_warmup_steps=warmup_steps, \n",
    "                                         num_training_steps=total_steps)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_model_path = f\"{run_dir}/best_model\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for input_values, batch_lbs in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_values)\n",
    "            loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'train_loss': f'{train_loss/(progress_bar.n+1):.4f}'})\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Log metrics\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f\"Train loss improved from {best_train_loss:.4f} to {train_loss:.4f}. Saving best model...\")\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    # Load the best model for final evaluation\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_d, batch_lbs in test_loader:\n",
    "            input_values = batch_d.to(device)\n",
    "            batch_lbs = batch_lbs.to(device)\n",
    "            \n",
    "            outputs = model(input_values)\n",
    "            loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "            test_loss += loss.item()\n",
    "            test_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_pred = np.array(test_pred).reshape(prepared_test_labels_timesteps.shape)\n",
    "    test_ground_truth = _get_ground_truth_labels(list(test_dict.values()), test_labels)\n",
    "    test_pred_flat = unsplit_data_ogsize(test_pred, window_size, step_sequence, 25, test_ground_truth.shape[-1])\n",
    "    test_prc_coef = _calculate_flattened_accuracy(test_pred_flat, test_ground_truth)\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Final Test Pearson Coefficient (flattened): {test_prc_coef:.4f}\")\n",
    "\n",
    "    # Log final test metrics\n",
    "    writer.add_scalar(\"Final/test_loss\", test_loss, 0)\n",
    "    writer.add_scalar(\"Final/test_pearson_coef\", test_prc_coef, 0)\n",
    "\n",
    "    # Log the final test metrics as a table\n",
    "    final_table = \"| Metric | Value |\\n\" \\\n",
    "                  \"|--------|-------|\\n\" \\\n",
    "                  f\"| Test Loss | {test_loss:.4f} |\\n\" \\\n",
    "                  f\"| Test Pearson Coefficient | {test_prc_coef:.4f} |\\n\"\n",
    "    writer.add_text(\"Final_Test_Metrics\", final_table)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Save final results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'Test_Loss': [test_loss],\n",
    "        'Test_Pearson_Coefficient': [test_prc_coef]\n",
    "    })\n",
    "    csv_path = os.path.join(run_dir, 'final_results.csv')\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## Path to data\n",
    "    path = \"/home/glenn/Downloads/\"\n",
    "    path = \"../DATA/\"\n",
    "\n",
    "\n",
    "    # Model parameters\n",
    "    model_config = {\n",
    "        \"VRBModel\": {\n",
    "            \"model\" : VRBModel,\n",
    "            \"model_name\": \"facebook/hubert-large-ls960-ft\",\n",
    "            \"hidden_units\": 64,\n",
    "            \"n_gru\": 3,\n",
    "            \"output_size\": None  # Will be set dynamically\n",
    "        },\n",
    "        \"Wav2Vec2ConvLSTMModel\": {\n",
    "            \"model\" : Wav2Vec2ConvLSTMModel,\n",
    "            \"model_name\": \"facebook/wav2vec2-base\",\n",
    "            \"hidden_units\": 128,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output_size\": None  # Will be set dynamically\n",
    "        },\n",
    "        \"RespBertLSTMModel\": {\n",
    "            'model': RespBertLSTMModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "        \"RespBertAttionModel\": {\n",
    "            'model' : RespBertAttionModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 512,\n",
    "            \"n_attion\": 2,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "            \"RespBertCNNModel\": {\n",
    "            'model' : RespBertCNNModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"output_size\": None  \n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    # Train and data parameters\n",
    "    epochs = 70\n",
    "    batch_size = 11\n",
    "    window_size = 30\n",
    "    step_size = 6\n",
    "    data_parts = 4 # aka folds\n",
    "    early_stopping_patience = 10\n",
    "    \n",
    "    config = model_config[\"RespBertCNNModel\"]\n",
    "    #model\n",
    "    \n",
    "    model = None\n",
    "\n",
    "    #processor = AutoProcessor.from_pretrained(config[\"model_name\"])\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "\n",
    "    train(\n",
    "        path_to_data=path+\"ComParE2020_Breathing/wav/\",\n",
    "        path_to_labels=path+\"ComParE2020_Breathing/lab/\",\n",
    "        window_size=window_size,\n",
    "        batch_size=batch_size,\n",
    "        config = config,\n",
    "        step_size=step_size,\n",
    "        data_parts= data_parts ,\n",
    "        early_stopping_patience= early_stopping_patience,\n",
    "        epochs= epochs,\n",
    "        model= model,\n",
    "        processor = processor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
