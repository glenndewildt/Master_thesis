{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1188, 480000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gdwildt/.conda/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'optimizer' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 268\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m#processor = AutoProcessor.from_pretrained(config[\"model_name\"])\u001b[39;00m\n\u001b[1;32m    265\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[38;5;241m.\u001b[39mfrom_pretrained(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 268\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mComParE2020_Breathing/wav/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mComParE2020_Breathing/lab/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(path_to_data, path_to_labels, window_size, step_size, early_stopping_patience, epochs, batch_size, config, model, processor)\u001b[0m\n\u001b[1;32m     96\u001b[0m t_mult \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     97\u001b[0m min_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-5\u001b[39m\n\u001b[0;32m---> 98\u001b[0m CosineAnnealingWarmRestarts(\u001b[43moptimizer\u001b[49m, T_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mt0, T_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mt_mult, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_lr)\n\u001b[1;32m     99\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m    101\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m*\u001b[39m epochs\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'optimizer' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torchaudio\n",
    "from typing import List, Tuple\n",
    "from pt_utils import *\n",
    "from pt_dataset import *\n",
    "from pt_models import *\n",
    "from pt_utils import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "def create_run_directory():\n",
    "    base_dir = \"pt_runs\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(base_dir, timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def _calculate_flattened_accuracy(average, ground_truth_labels):\n",
    "    s_acc = 0\n",
    "    for b in range(len(ground_truth_labels)):\n",
    "        s, _ = scipy.stats.pearsonr(average[b], ground_truth_labels[b])\n",
    "        s_acc += s\n",
    "    return s_acc / len(ground_truth_labels)\n",
    "\n",
    "def _choose_real_labs_only_with_filenames(labels, filenames):\n",
    "    return labels[labels['filename'].isin(filenames)]\n",
    "\n",
    "def _get_ground_truth_labels(ground_truth_names, labels):\n",
    "    ground_truth_labels = []\n",
    "    for batch_name in ground_truth_names:\n",
    "        ground_truth_label = _choose_real_labs_only_with_filenames(labels, [batch_name])\n",
    "        ground_truth_labels.append(ground_truth_label)\n",
    "    return np.array(ground_truth_labels)[:, :, -1].astype(np.float32)\n",
    "\n",
    "def train(path_to_data, path_to_labels, window_size=16, step_size=6, early_stopping_patience=10,epochs=100, batch_size=10, config=None, model=None, processor=None):\n",
    "    run_dir = create_run_directory()\n",
    "    log_dir = os.path.join(run_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Parameters\n",
    "    length_sequence = window_size \n",
    "    step_sequence = step_size\n",
    "\n",
    "    # Load and prepare data\n",
    "    train_data, train_labels, train_dict, frame_rate = load_data(path_to_data, path_to_labels, 'train')\n",
    "    devel_data, devel_labels, devel_dict, frame_rate = load_data(path_to_data, path_to_labels, 'devel')\n",
    "    test_data, test_labels, test_dict, frame_rate = load_data(path_to_data, path_to_labels, 'test')\n",
    "\n",
    "    # Combine train and devel data\n",
    "    all_data = np.concatenate((train_data, devel_data), axis=0)\n",
    "    all_labels = pd.concat([train_labels, devel_labels])\n",
    "    all_dict = np.concatenate((list(train_dict.values()), list(devel_dict.values())), axis=0)\n",
    "\n",
    "    # Prepare data\n",
    "    prepared_data, prepared_labels, prepared_labels_timesteps = prepare_data(all_data, all_labels, all_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "    prepared_test_data, prepared_test_labels, prepared_test_labels_timesteps = prepare_data(test_data, test_labels, test_dict, frame_rate, length_sequence * 16000, step_sequence * 16000)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    config[\"output_size\"] = prepared_labels.shape[-1]\n",
    "    writer = SummaryWriter(log_dir=os.path.join(log_dir, config[\"model_name\"]))\n",
    "\n",
    "    # Reshape data\n",
    "    train_d, train_lbs = reshaping_data_for_model(prepared_data, prepared_labels)\n",
    "    test_d, test_lbs = reshaping_data_for_model(prepared_test_data, prepared_test_labels)\n",
    "    \n",
    "    print(train_d.shape)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = GPUBreathingDataset(train_d, train_lbs, processor, augment=True)\n",
    "    test_dataset = BreathingDataset(test_d, test_lbs, processor, window_size, step_sequence)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=5, collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "    # Create and initialize model\n",
    "    model = config[\"model\"](config).to(device)\n",
    "\n",
    "    # Optimizer and scheduler setup\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 0.001\n",
    "    t0 = 10\n",
    "    t_mult = 2\n",
    "    min_lr = 2e-5\n",
    "    CosineAnnealingWarmRestarts(optimizer, T_0=self.config.t0, T_mult=self.config.t_mult, eta_min=self.config.min_lr)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                         num_warmup_steps=warmup_steps, \n",
    "                                         num_training_steps=total_steps)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_model_path = f\"{run_dir}/best_model\"\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for input_values, batch_lbs in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_values)\n",
    "            loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'train_loss': f'{train_loss/(progress_bar.n+1):.4f}'})\n",
    "            scheduler.step()\n",
    "                        # Clear cache\n",
    "            del input_values, batch_lbs, outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Log metrics\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f\"train loss improved from {best_train_loss:.4f} to {train_loss:.4f}. Saving best model...\")\n",
    "            best_train_loss = train_loss\n",
    "            early_stopping_counter = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"train loss did not improve for {early_stopping_counter} epochs.\")\n",
    "            #model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}. Loading best model.\")\n",
    "            # Load the best model's weights\n",
    "            #model.load_state_dict(torch.load(best_model_path))\n",
    "            break\n",
    "\n",
    "    # Load the best model for final evaluation\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_d, batch_lbs in test_loader:\n",
    "            input_values = batch_d.to(device)\n",
    "            batch_lbs = batch_lbs.to(device)\n",
    "            \n",
    "            outputs = model(input_values)\n",
    "            loss = correlation_coefficient_loss(outputs, batch_lbs)\n",
    "            test_loss += loss.item()\n",
    "            test_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_pred = np.array(test_pred).reshape(prepared_test_labels_timesteps.shape)\n",
    "    test_ground_truth = _get_ground_truth_labels(list(test_dict.values()), test_labels)\n",
    "    test_pred_flat = unsplit_data_ogsize(test_pred, window_size, step_sequence, 25, test_ground_truth.shape[-1])\n",
    "    test_prc_coef = _calculate_flattened_accuracy(test_pred_flat, test_ground_truth)\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Final Test Pearson Coefficient (flattened): {test_prc_coef:.4f}\")\n",
    "\n",
    "    # Log final test metrics\n",
    "    writer.add_scalar(\"Final/test_loss\", test_loss, 0)\n",
    "    writer.add_scalar(\"Final/test_pearson_coef\", test_prc_coef, 0)\n",
    "\n",
    "    # Log the final test metrics as a table\n",
    "    final_table = \"| Metric | Value |\\n\" \\\n",
    "                  \"|--------|-------|\\n\" \\\n",
    "                  f\"| Test Loss | {test_loss:.4f} |\\n\" \\\n",
    "                  f\"| Test Pearson Coefficient | {test_prc_coef:.4f} |\\n\"\n",
    "    writer.add_text(\"Final_Test_Metrics\", final_table)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Save final results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'Test_Loss': [test_loss],\n",
    "        'Test_Pearson_Coefficient': [test_prc_coef]\n",
    "    })\n",
    "    csv_path = os.path.join(run_dir, 'final_results.csv')\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## Path to data\n",
    "    path = \"/home/glenn/Downloads/\"\n",
    "    path = \"../DATA/\"\n",
    "\n",
    "\n",
    "    # Model parameters\n",
    "    model_config = {\n",
    "        \"VRBModel\": {\n",
    "            \"model\" : VRBModel,\n",
    "            \"model_name\": \"facebook/hubert-large-ls960-ft\",\n",
    "            \"hidden_units\": 64,\n",
    "            \"n_gru\": 3,\n",
    "            \"output_size\": None  # Will be set dynamically\n",
    "        },\n",
    "        \"Wav2Vec2ConvLSTMModel\": {\n",
    "            \"model\" : Wav2Vec2ConvLSTMModel,\n",
    "            \"model_name\": \"facebook/wav2vec2-base\",\n",
    "            \"hidden_units\": 128,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output_size\": None  # Will be set dynamically\n",
    "        },\n",
    "        \"RespBertLSTMModel\": {\n",
    "            'model': RespBertLSTMModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"n_lstm\": 2,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "        \"RespBertAttionModel\": {\n",
    "            'model' : RespBertAttionModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 512,\n",
    "            \"n_attion\": 2,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "            \"RespBertCNNModel\": {\n",
    "            'model' : RespBertCNNModel,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"output_size\": None  \n",
    "        },\n",
    "            \"RespBertCNN_12_Model\": {\n",
    "            'model' : RespBertCNN_12_Model,\n",
    "            \"model_name\": \"microsoft/wavlm-large\",\n",
    "            \"hidden_units\": 256,\n",
    "            \"output_size\": None \n",
    "            },\n",
    "            \n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    # Train and data parameters\n",
    "    epochs = 70\n",
    "    batch_size = 13\n",
    "    window_size = 30\n",
    "    step_size = 6\n",
    "    early_stopping_patience = 10\n",
    "    \n",
    "    config = model_config[\"RespBertCNNModel\"]\n",
    "    #model\n",
    "    \n",
    "    model = None\n",
    "\n",
    "    #processor = AutoProcessor.from_pretrained(config[\"model_name\"])\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "\n",
    "    train(\n",
    "        path_to_data=path+\"ComParE2020_Breathing/wav/\",\n",
    "        path_to_labels=path+\"ComParE2020_Breathing/lab/\",\n",
    "        window_size=window_size,\n",
    "        batch_size=batch_size,\n",
    "        config = config,\n",
    "        step_size=step_size,\n",
    "        early_stopping_patience= early_stopping_patience,\n",
    "        epochs= epochs,\n",
    "        model= model,\n",
    "        processor = processor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
